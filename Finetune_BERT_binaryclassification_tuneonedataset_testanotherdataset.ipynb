{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqqucYv6tNFe"
      },
      "outputs": [],
      "source": [
        "#This code was written to finetune BERT to identify hedges (binary classification), finetuning on one domain and testing on another.\n",
        "!pip install datasets evaluate transformers[torch] accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "import pandas as pd\n",
        "import os\n",
        "from datasets import load_dataset, Dataset\n",
        "from sklearn.model_selection import KFold\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "import evaluate\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import shuffle"
      ],
      "metadata": {
        "id": "kXQuki-mm8zR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ly0Qf96BD-P"
      },
      "outputs": [],
      "source": [
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOMrLyTAHalR"
      },
      "outputs": [],
      "source": [
        "%cd /content/your_directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz_Lm4GhtYOA"
      },
      "outputs": [],
      "source": [
        "# Load dataset a\n",
        "ds_a = load_dataset('csv', data_files=\"dataset_a.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK0aLFJxBUmA"
      },
      "outputs": [],
      "source": [
        "print(ds_a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R8wakuQwjjS"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
        "#or tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6B3S1_5wxWf"
      },
      "outputs": [],
      "source": [
        "#define tokenizer function\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(\n",
        "      examples[\"text\"], padding=\"max_length\", truncation=True\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6IP51_9wYiP"
      },
      "outputs": [],
      "source": [
        "#tokenize dataset\n",
        "tokenized_dataset_a = ds_a.map(\n",
        "    tokenize_function, batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujPib6dlSrse"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset_a)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vMhNUVlIcu-I"
      },
      "outputs": [],
      "source": [
        "#make dataframe\n",
        "dataseta2 = tokenized_dataset_a['train']\n",
        "df_a = dataseta2.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsNCibbXxQFX"
      },
      "outputs": [],
      "source": [
        "print(df_a.head())\n",
        "print(df_a.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjmbK5ykN1l1"
      },
      "outputs": [],
      "source": [
        "  #Split kfolds and make folder to save models in\n",
        "  n=5\n",
        "  kf = KFold(n_splits=n, random_state=42, shuffle=True)\n",
        "  model_save_dir = \"./directory_for_saving_models\"\n",
        "  os.makedirs(model_save_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9Qcqb7TgwDU"
      },
      "outputs": [],
      "source": [
        "print(kf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX_arvGbW-V4"
      },
      "outputs": [],
      "source": [
        "#load accuracy, f1, recall, and mcc and store in variable\n",
        "accuracy_metric = evaluate.load(\"accuracy\")\n",
        "f1_metric = evaluate.load(\"f1\")\n",
        "recall_metric = evaluate.load(\"recall\")\n",
        "matthews_metric = evaluate.load(\"matthews_correlation\")\n",
        "precision_metric = evaluate.load(\"precision\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vtaQ9TtyCz0"
      },
      "outputs": [],
      "source": [
        "#define compute metrics\n",
        "def compute_metrics(eval_pred):\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=-1)\n",
        "  # store accuracy, f1 and recall and store in variables\n",
        "  accuracy = accuracy_metric.compute(predictions=predictions, references=labels)['accuracy']\n",
        "  f1 = f1_metric.compute(predictions=predictions, references=labels)['f1']\n",
        "  recall = recall_metric.compute(predictions=predictions, references=labels)['recall']\n",
        "  precision = precision_metric.compute(predictions=predictions, references=labels)['precision']\n",
        "  mcc = matthews_metric.compute(predictions=predictions, references=labels)['matthews_correlation']\n",
        "  #tell the computer you want each metric and that you want it labeled accordingly\n",
        "  return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'recall': recall,\n",
        "        'precision': precision,\n",
        "        'mcc': mcc\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAjwixkk-1tN"
      },
      "outputs": [],
      "source": [
        "#save training metric logs\n",
        "class DetailedTrainingLogger(TrainerCallback):\n",
        "    def __init__(self, trainer, eval_dataset):\n",
        "        self.logs = []\n",
        "        self.trainer = trainer\n",
        "        self.eval_dataset = eval_dataset\n",
        "\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Access logs directly without disrupting default behavior\n",
        "        if logs:\n",
        "            logs_entry = {\n",
        "                \"Step\": state.global_step,\n",
        "                \"Training Loss\": logs.get(\"loss\", \"No log\"),\n",
        "                \"Validation Loss\": logs.get(\"eval_loss\", None),\n",
        "                \"Accuracy\": logs.get(\"eval_accuracy\", None),\n",
        "                \"F1\": logs.get(\"eval_f1\", None),\n",
        "                \"Recall\": logs.get(\"eval_recall\", None),\n",
        "                \"Precision\": logs.get(\"eval_precision\", None),\n",
        "                \"MCC\": logs.get(\"eval_mcc\", None),\n",
        "            }\n",
        "            self.logs.append(logs_entry)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmYNzlZxCuHJ"
      },
      "outputs": [],
      "source": [
        "#define training arguments; adjust as needed\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"bert_trainer\",\n",
        "    run_name='name',\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50, logging_steps=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    num_train_epochs=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',\n",
        "    greater_is_better=True,\n",
        "    report_to=\"none\",\n",
        "    save_steps=50,\n",
        "    seed=42\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoCQ0rKlQM_R"
      },
      "outputs": [],
      "source": [
        "#create a metrics dataframe to save metrics info later\n",
        "metrics_df = pd.DataFrame(columns=['fold', 'accuracy', 'f1', 'recall', 'precision', 'mcc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i75PSKy42G1s"
      },
      "outputs": [],
      "source": [
        "#make metrics list\n",
        "metrics_list = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gxWHLgYJxT-"
      },
      "outputs": [],
      "source": [
        "#make list to fill with incorrectly predicted sentences\n",
        "incorrect_list = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#create list to store logs\n",
        "all_logs = []"
      ],
      "metadata": {
        "id": "JG15pXxOqcyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0CmHt-Ivcwa"
      },
      "outputs": [],
      "source": [
        "fold_num = 1  # Initialize fold counter\n",
        "\n",
        "# Initialize lists to accumulate predictions and true labels from all folds\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "\n",
        "for train_index, val_index in kf.split(df_a):\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-german-cased\", num_labels=2)\n",
        "    #or model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2) or other model\n",
        "    #send to gpu\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    # splitting Dataframe\n",
        "    train_df = df_a.iloc[train_index]\n",
        "    val_df = df_a.iloc[val_index]\n",
        "\n",
        "    # Convert back into dataset for trainer\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    eval_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "    # Training arguments\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "    # Create DetailedTrainingLogger with the trainer and eval_dataset\n",
        "    detailed_logger = DetailedTrainingLogger(trainer, eval_dataset)\n",
        "\n",
        "    # Add the logger to the trainer's callbacks\n",
        "    trainer.add_callback(detailed_logger)\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Add logs to the aggregated list, including fold number\n",
        "    for log in detailed_logger.logs:\n",
        "        log[\"Fold\"] = fold_num\n",
        "        all_logs.append(log)\n",
        "\n",
        "    #Save model\n",
        "    model_directory =f\"{model_save_dir}/model_directory_{fold_num}\"\n",
        "    model.save_pretrained(model_directory)\n",
        "\n",
        "    # Evaluate\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Get predictions\n",
        "    outputs = trainer.predict(eval_dataset)\n",
        "    predictions = np.argmax(outputs.predictions, axis=1)\n",
        "    true_labels = np.array(eval_dataset['label'])\n",
        "\n",
        "    # Accumulate predictions and true labels\n",
        "    all_predictions.extend(predictions)\n",
        "    all_true_labels.extend(true_labels)\n",
        "\n",
        "\n",
        "    # Collect metrics\n",
        "    metrics = compute_metrics((outputs.predictions, eval_dataset['label']))\n",
        "\n",
        "    selected_metrics = {\n",
        "    'Fold': fold_num,\n",
        "    'accuracy': metrics.get('accuracy', None),\n",
        "    'f1': metrics.get('f1', None),\n",
        "    'recall': metrics.get('recall', None),\n",
        "    'precision': metrics.get('precision', None),\n",
        "    'mcc': metrics.get('mcc', None)\n",
        "}\n",
        "    metrics_list.append(selected_metrics)\n",
        "\n",
        "    # Increase fold count\n",
        "    fold_num += 1\n",
        "\n",
        "    # Identify misclassified examples\n",
        "    misclassified_indices = np.where(predictions != true_labels)[0]\n",
        "    misclassified_examples = val_df.iloc[misclassified_indices]\n",
        "\n",
        "    # Print misclassified examples\n",
        "    if len(misclassified_indices) > 0:\n",
        "        for idx in range(len(misclassified_examples)):\n",
        "            incorrect = {\n",
        "                'text id': misclassified_examples.iloc[idx]['text id'],\n",
        "                'text': misclassified_examples.iloc[idx]['text'],\n",
        "                'True Label': true_labels[misclassified_indices[idx]],\n",
        "                'Prediction': predictions[misclassified_indices[idx]]\n",
        "            }\n",
        "            incorrect_list.append(incorrect)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoLqG4kaoS2X"
      },
      "outputs": [],
      "source": [
        "# Convert metrics to DataFrame\n",
        "metrics_df = pd.DataFrame(metrics_list)\n",
        "# Save to CSV\n",
        "csv_file_path = '/content/drive/your_metrics_folder/name.csv'\n",
        "metrics_df.to_csv(csv_file_path, index=False)\n",
        "print(f'Metrics saved to {csv_file_path}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgOAZvZAXidy"
      },
      "outputs": [],
      "source": [
        "# Create dataframe of training logs and save\n",
        "training_log_df = pd.DataFrame(all_logs)\n",
        "log_csv_path = '/content/drive/your_metrics_folder/training_log.csv'\n",
        "training_log_df.to_csv(log_csv_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_Ei4OH4h1_S"
      },
      "outputs": [],
      "source": [
        "# Calculate mean and standard deviation for each metric across folds\n",
        "metrics_mean = metrics_df.mean()\n",
        "metrics_std = metrics_df.std()\n",
        "\n",
        "# Add mean and std to the DataFrame for reference\n",
        "summary_df = pd.DataFrame({\n",
        "    'metric': metrics_mean.index,\n",
        "    'mean': metrics_mean.values,\n",
        "    'std': metrics_std.values\n",
        "})\n",
        "\n",
        "# Save the summary of mean and std\n",
        "summary_csv_path = '/content/drive/your_metrics_folder/metrics_summary.csv'\n",
        "summary_df.to_csv(summary_csv_path, index=False)\n",
        "\n",
        "print(f'Metrics saved to {csv_file_path}')\n",
        "print(f'Summary of mean and std saved to {summary_csv_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiagyHbNKwH5"
      },
      "outputs": [],
      "source": [
        "sentences_df = pd.DataFrame(columns=['text id','text', 'true label', 'prediction',])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uup3oOXFLF2R"
      },
      "outputs": [],
      "source": [
        "# Convert sentences, labels and predictions to DataFrame\n",
        "sentences_df = pd.DataFrame(incorrect_list)\n",
        "# Save to CSV\n",
        "csv_file_path = '/content/drive/your_metrics_folder/incorrect_sentences.csv'\n",
        "sentences_df.to_csv(csv_file_path, index=False)\n",
        "print(f'Sentences saved to {csv_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeFOslxnPcQO"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Turn predictions into numpy array and store in variable\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_true_labels = np.array(all_true_labels)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "overall_cm = confusion_matrix(all_true_labels, all_predictions)\n",
        "\n",
        "# Convert the confusion matrix to a DataFrame for better readability\n",
        "cm_df = pd.DataFrame(overall_cm,\n",
        "                     index=['True Negative', 'True Positive'],\n",
        "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "# Save the confusion matrix to a CSV file\n",
        "csv_file_path = '/content/drive/your_metrics_folder/matrix_dataseta.csv'\n",
        "cm_df.to_csv(csv_file_path)\n",
        "print(f\"Confusion matrix saved to {csv_file_path}\")\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
        "sns.heatmap(cm_df,\n",
        "            annot=True,           # Annotate each cell with the numeric value\n",
        "            fmt='d',              # Format for integer numbers\n",
        "            cmap='Blues',         # Color map for the heatmap\n",
        "            linewidths=0.5,       # Line widths between cells\n",
        "            linecolor='white')    # Line color between cells\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Your Title\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "\n",
        "\n",
        "# Save the heatmap as an image file\n",
        "heatmap_file_path = '/content/drive/your_metrics_folder/heatmap_dataseta.png'\n",
        "plt.tight_layout()  # Adjust layout to avoid clipping of labels\n",
        "plt.savefig(heatmap_file_path, dpi=300, bbox_inches='tight')  # Save the plot\n",
        "print(f\"Heatmap saved to {heatmap_file_path}\")\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()  # Adjust layout to avoid clipping of labels\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Om9X2cnPxA"
      },
      "outputs": [],
      "source": [
        "#load second dataset\n",
        "ds_b= load_dataset('csv', data_files=\"datasetb.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLvchKbKnM8d"
      },
      "outputs": [],
      "source": [
        "print(ds_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSKOaYsbnXU0"
      },
      "outputs": [],
      "source": [
        "#tokenize second dataset\n",
        "tokenized_dataset_b = ds_b.map(\n",
        "    tokenize_function, batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXkH99cLnXZ8"
      },
      "outputs": [],
      "source": [
        "print(tokenized_dataset_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqYHFBHloXnK"
      },
      "outputs": [],
      "source": [
        "#Create dataframe\n",
        "dataset_b_2 = tokenized_dataset_b['train']\n",
        "df_b = dataset_b_2.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HN_BdIIpCKC"
      },
      "outputs": [],
      "source": [
        "#Shuffle data\n",
        "df_b = shuffle(df_b, random_state=42).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU3cLnpSs-tq"
      },
      "outputs": [],
      "source": [
        "#create metrics dataframe\n",
        "metrics_df_b = pd.DataFrame(columns=['fold', 'accuracy', 'f1', 'recall', 'precision', 'mcc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhqCUEPgvBdN"
      },
      "outputs": [],
      "source": [
        "#make metrics list\n",
        "metrics_list_b = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Sk0O_BvMqP"
      },
      "outputs": [],
      "source": [
        "#create list for incorrectly predicted sentences\n",
        "incorrect_list_b = []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#new \"training\" arguments- set training to no\n",
        "training_argsb = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"no\",\n",
        "    save_strategy=\"no\",\n",
        "    do_train=False,\n",
        "    report_to=\"none\",\n",
        "    disable_tqdm=False,\n",
        ")"
      ],
      "metadata": {
        "id": "PFTdHRlXH-Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0B9lpsnLtT"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_DISABLED\"] = \"true\"  # Disable W&B for the current session\n",
        "\n",
        "\n",
        "# Accumulate predictions and true labels across folds\n",
        "all_predictions_b = []\n",
        "all_true_labels_b = []\n",
        "\n",
        "for fold_num in range(1, 6):  # Loop through each saved model (5 folds)\n",
        "    # Load the saved model\n",
        "    model_directory = f\"{model_save_dir}/model_directory_{fold_num}\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_directory, num_labels=2)\n",
        "    #send to gpu\n",
        "    model.to(\"cuda\")\n",
        "\n",
        "    # Initialize Trainer for evaluation\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_argsb,\n",
        "        eval_dataset=dataset_b_2,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Evaluate the model\n",
        "    eval_results = trainer.evaluate()\n",
        "\n",
        "    # Predict on Dataset B\n",
        "    outputs = trainer.predict(dataset_b_2)\n",
        "    predictions = np.argmax(outputs.predictions, axis=1)\n",
        "    true_labels = dataset_b_2['label']\n",
        "\n",
        "    # Accumulate predictions and true labels\n",
        "    all_predictions_b.extend(predictions)\n",
        "    all_true_labels_b.extend(true_labels)\n",
        "\n",
        "    # Store evaluation metrics\n",
        "    selected_metrics = {\n",
        "        'Fold': fold_num,\n",
        "        'accuracy': eval_results.get('eval_accuracy', None),\n",
        "        'f1': eval_results.get('eval_f1', None),\n",
        "        'recall': eval_results.get('eval_recall', None),\n",
        "        'precision': eval_results.get('eval_precision', None),\n",
        "        'mcc': eval_results.get('eval_mcc', None),\n",
        "    }\n",
        "    metrics_list_b.append(selected_metrics)\n",
        "\n",
        "    # Identify misclassified examples\n",
        "    misclassified_indices = np.where(predictions != true_labels)[0]\n",
        "    for idx in misclassified_indices:\n",
        "          # Explicitly convert idx to a Python int\n",
        "          idx = int(idx)\n",
        "          incorrect = {\n",
        "              'column 1': dataset_b_2[idx]['column 1'],\n",
        "              'column 2': dataset_b_2[idx]['column 2'],\n",
        "              'True Label': true_labels[idx],\n",
        "              'Prediction': predictions[idx],\n",
        "              }\n",
        "          incorrect_list_b.append(incorrect)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfEpgZNSzYow"
      },
      "outputs": [],
      "source": [
        "# Convert metrics to DataFrame\n",
        "metrics_df_b = pd.DataFrame(metrics_list_b)\n",
        "# Save to CSV\n",
        "csv_file_path = '/content/drive/your_metrics_folder/metrics_datasetb.csv'\n",
        "metrics_df_b.to_csv(csv_file_path, index=False)\n",
        "print(f'Metrics saved to {csv_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqSL3Hk7zrPb"
      },
      "outputs": [],
      "source": [
        "# Calculate mean and standard deviation for each metric across folds\n",
        "metrics_mean_b = metrics_df_b.mean()\n",
        "metrics_std_b = metrics_df_b.std()\n",
        "\n",
        "# Add mean and std to the DataFrame for reference\n",
        "summary_df_b = pd.DataFrame({\n",
        "    'metric': metrics_mean_b.index,\n",
        "    'mean': metrics_mean_b.values,\n",
        "    'std': metrics_std_b.values\n",
        "})\n",
        "\n",
        "# Save the summary of mean and std\n",
        "summary_csv_path = '/content/drive/your_metrix_folder/metrics_summary_datasetb.csv'\n",
        "summary_df_b.to_csv(summary_csv_path, index=False)\n",
        "\n",
        "print(f'Metrics saved to {csv_file_path}')\n",
        "print(f'Summary of mean and std saved to {summary_csv_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsOBZhtx0B9b"
      },
      "outputs": [],
      "source": [
        "sentences_df = pd.DataFrame(columns=['text id','text', 'true label', 'prediction',])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jylOB8s0VJO"
      },
      "outputs": [],
      "source": [
        "# Convert sentences, labels and predictions to DataFrame\n",
        "sentences_df_b = pd.DataFrame(incorrect_list_b)\n",
        "# Save to CSV\n",
        "csv_file_path = '/content/drive/your_metrics_folder/misidentified_sentences_datsetb.csv'\n",
        "sentences_df_b.to_csv(csv_file_path, index=False)\n",
        "print(f'Sentences saved to {csv_file_path}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf4JB9In0fw9"
      },
      "outputs": [],
      "source": [
        "# Create array from predictions and labels\n",
        "all_predictions_b = np.array(all_predictions_b)\n",
        "all_true_labels_b = np.array(all_true_labels_b)\n",
        "\n",
        "# Generate the confusion matrix\n",
        "overall_cmb = confusion_matrix(all_true_labels_b, all_predictions_b)\n",
        "\n",
        "# Convert the confusion matrix to a DataFrame for better readability\n",
        "cm_df = pd.DataFrame(overall_cmb,\n",
        "                     index=['True Negative', 'True Positive'],\n",
        "                     columns=['Predicted Negative', 'Predicted Positive'])\n",
        "\n",
        "# Save the confusion matrix to a CSV file\n",
        "csv_file_path = '/content/drive/your_metrics_folder/matrix_dtasetb.csv'\n",
        "cm_df.to_csv(csv_file_path)\n",
        "print(f\"Confusion matrix saved to {csv_file_path}\")\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(8, 6))  # Adjust the figure size as needed\n",
        "sns.heatmap(cm_df,\n",
        "            annot=True,           # Annotate each cell with the numeric value\n",
        "            fmt='d',              # Format for integer numbers\n",
        "            cmap='Blues',         # Color map for the heatmap\n",
        "            linewidths=0.5,       # Line widths between cells\n",
        "            linecolor='white')    # Line color between cells\n",
        "\n",
        "# Add labels and title\n",
        "plt.title(\"Dataset B\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "\n",
        "# Show the plot\n",
        "plt.tight_layout()  # Adjust layout to avoid clipping of labels\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}